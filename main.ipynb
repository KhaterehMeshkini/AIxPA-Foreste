{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13113b30-4d11-4e53-9c46-520280e3c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=os.environ['S3_ENDPOINT_URL'],\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']\n",
    ")\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20180129T101251_N0500_R022_T32TPS_20230904T215802.SAFE.zip', 'DATA/S2A_MSIL2A_20180129T101251_N0500_R022_T32TPS_20230904T215802.zip')\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20180422T102031_N0500_R065_T32TPS_20230915T161912.SAFE.zip', 'DATA/S2A_MSIL2A_20180422T102031_N0500_R065_T32TPS_20230915T161912.zip')\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20180926T101021_N0500_R022_T32TPS_20230729T180843.SAFE.zip', 'DATA/S2A_MSIL2A_20180926T101021_N0500_R022_T32TPS_20230729T180843.zip')\n",
    "\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20190328T102021_N0500_R065_T32TPS_20221115T132105.SAFE.zip', 'DATA/S2A_MSIL2A_20190328T102021_N0500_R065_T32TPS_20221115T132105.zip')\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20190626T102031_N0500_R065_T32TPS_20230723T161149.SAFE.zip', 'DATA/S2A_MSIL2A_20190626T102031_N0500_R065_T32TPS_20230723T161149.zip')\n",
    "s3.download_file('datalake', 'deforestation/datatn/files/S2A_MSIL2A_20190904T102021_N0500_R065_T32TPS_20230703T073148.SAFE.zip', 'DATA/S2A_MSIL2A_20190904T102021_N0500_R065_T32TPS_20230703T073148.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee790946-b1a1-4f7c-b8bc-068fbde8f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install gdal -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d83ea6-251b-4292-a9e9-96cbd477bb08",
   "metadata": {},
   "source": [
    "### Deforestation Processing Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f3cbb-6481-4a3a-93fe-69c0a656527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sys, os, time, shutil, json\n",
    "from os.path import abspath\n",
    "import utils.filemanager as fm\n",
    "from utils.S2L2A import L2Atile, getTileList\n",
    "from utils.utils import _ndi\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, cpu_count, delayed\n",
    "from utils.utils import run_bfast_parallel, get_month_numbers, interpolate_for_year, interpolate_time_series, fuse_features, process_pixel\n",
    "import utils.post_processing as pp\n",
    "\n",
    "\n",
    "def deforestation(sensor, tilename, years, maindir, datapath, outpath):\n",
    "    # Initialize logging and timer\n",
    "    logging = {} \n",
    "    t_tot = time.time()\n",
    "\n",
    "    # Check sensor type and get tile list\n",
    "    if sensor == 'S2':\n",
    "        tiledict = getTileList(datapath)\n",
    "    else:\n",
    "        raise IOError('Invalid sensor')\n",
    "    \n",
    "    keys = tiledict.keys()\n",
    "\n",
    "    for k in keys:\n",
    "        tileDatapath = tiledict[k]\n",
    "        print(f\"Reading Tile-{k}.\")\n",
    "        \n",
    "        if sensor == 'S2':\n",
    "            tile = L2Atile(maindir, tileDatapath)\n",
    "\n",
    "        # Initialize empty storage for all years\n",
    "        feature_NDVI_all = None\n",
    "        feature_BSI_all = None\n",
    "        all_dates = []    \n",
    "\n",
    "        for y in years:\n",
    "            # Set temporary path for the current year\n",
    "            temppath = fm.joinpath(maindir, 'numpy', tilename)\n",
    "\n",
    "            # Get features for the current year\n",
    "            ts, _, _ = tile.gettimeseries(year=y, option='default')\n",
    "            fn = [f for f in os.listdir(temppath)] \n",
    "\n",
    "            if len(ts) != 0:\n",
    "                print(f'Extracting features for each image for year {y}:')\n",
    "            \n",
    "            # Get some information from data\n",
    "            height, width = ts[0].feature('B04').shape\n",
    "            geotransform, projection = fm.getGeoTIFFmeta(ts[0].featurepath()['B04'])\n",
    "            ts_length = len(ts)\n",
    "\n",
    "            ts = sorted(ts, key=lambda x: x.InvalidPixNum())[0:ts_length]\n",
    "            totimg = len(ts)\n",
    "            totfeature = 2\n",
    "            feature_NDVI = np.empty((height, width, totimg))\n",
    "            feature_BSI = np.empty((height, width, totimg))\n",
    "            dates = []\n",
    "               \n",
    "            # Compute Index Statistics\n",
    "            for idx, img in enumerate(ts):        \n",
    "                print(f'.. {idx+1}/{totimg}      ', end='\\r')   \n",
    "                        \n",
    "                # Compute NDVI and BSI indices\n",
    "                b3 = img.feature('RED', dtype=np.float16)\n",
    "                b4 = img.feature('nir', dtype=np.float16)\n",
    "                b5 = img.feature('SWIR1', dtype=np.float16)\n",
    "                date = img._metadata['date']\n",
    "                dates.append(date)\n",
    "    \n",
    "                NDVI = _ndi(b4, b3)\n",
    "                BSI = _ndi(b4, b5)\n",
    "    \n",
    "                # Mask for valid values (update if needed)\n",
    "                fn = fn[1:]\n",
    "                name = fn[idx]\n",
    "                maskpath = fm.joinpath(temppath, name, 'MASK.npy')\n",
    "                msk = np.load(maskpath)\n",
    "\n",
    "                NDVI_mask = np.where(msk, np.nan, NDVI)\n",
    "                BSI_mask = np.where(msk, np.nan, BSI)\n",
    "\n",
    "\n",
    "                feature_NDVI[:, :, idx] = NDVI_mask\n",
    "                feature_BSI[:, :, idx] = BSI_mask\n",
    "\n",
    "                # Delete intermediate arrays to free memory\n",
    "                del b3, b4, b5, NDVI, BSI, msk\n",
    "\n",
    "            # Append yearly features and dates\n",
    "            feature_NDVI_all = np.concatenate((feature_NDVI_all, feature_NDVI), axis=-1) if feature_NDVI_all is not None else feature_NDVI\n",
    "            feature_BSI_all = np.concatenate((feature_BSI_all, feature_BSI), axis=-1) if feature_BSI_all is not None else feature_BSI\n",
    "            all_dates.extend(dates)\n",
    "            print(all_dates)\n",
    "\n",
    "    #read the dates\n",
    "    # Convert the date strings to datetime objects\n",
    "    all_dates_datetime = [datetime.strptime(date, '%Y%m%d') for date in all_dates]\n",
    "    \n",
    "    # Separate dates based on the year\n",
    "    dates_2018 = [date for date in all_dates_datetime if date.year == 2018]\n",
    "    dates_2019 = [date for date in all_dates_datetime if date.year == 2019]\n",
    "\n",
    "    \n",
    "    #NDVI & BSI data\n",
    "    ndvi_data = feature_NDVI_all\n",
    "    bsi_data = feature_BSI_all\n",
    "    \n",
    "    height, width, _ = ndvi_data.shape\n",
    "    interpolated_data = np.zeros((height, width, 24), dtype=np.float16)\n",
    "    \n",
    "\n",
    "    ndvi_data = np.asarray(ndvi_data, dtype=np.float16)\n",
    "    bsi_data = np.asarray(bsi_data, dtype=np.float16)\n",
    "    \n",
    "    '''\n",
    "    # Vectorized approach for fusion\n",
    "    interpolated_ndvi = np.apply_along_axis(interpolate_time_series, 2, ndvi_data, dates_2018, dates_2019)\n",
    "    interpolated_bsi = np.apply_along_axis(interpolate_time_series, 2, bsi_data, dates_2018, dates_2019)\n",
    "\n",
    "    del ndvi_data, bsi_data\n",
    "    \n",
    "    # Fuse features across all pixels in a vectorized way\n",
    "    interpolated_data = fuse_features(interpolated_ndvi, interpolated_bsi).astype(np.float16)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Run parallel processing fusion across pixels\n",
    "    results = Parallel(n_jobs=-1, backend='loky')(\n",
    "        delayed(process_pixel)(i, j) for i in range(height) for j in range(width)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Store results back into the array\n",
    "    for i, j, fused_pixel in results:\n",
    "        interpolated_data[i, j, :] = fused_pixel\n",
    "\n",
    "    # Reshape for BFAST\n",
    "    totpixels = height * width\n",
    "    fused_reshaped = interpolated_data.reshape((totpixels, 24))\n",
    "   \n",
    "    \n",
    "    # Run BFAST\n",
    "    startyear = int(years[0])\n",
    "    endyear = int(years[-1]) \n",
    "    freq = 12 #monthly data\n",
    "    nyear = endyear - startyear \n",
    "    years_np = np.arange(startyear, endyear+1)\n",
    "    \n",
    "    \n",
    "    with Parallel(n_jobs=-1) as parallel:\n",
    "        dates = bfast.r_style_interval((startyear, 1), (startyear + nyear, 365), freq).reshape(fused_reshaped.shape[1], 1)\n",
    "        breaks, confidence = run_bfast_parallel(parallel, fused_reshaped, dates, freq)\n",
    "          \n",
    "    # Process results\n",
    "    changemaps = breaks // freq\n",
    "    accuracymaps = confidence\n",
    "    changemaps = changemaps.reshape(height, width)\n",
    "    accuracymaps = accuracymaps.reshape(height, width)\n",
    "    \n",
    "    \n",
    "    \n",
    "    changemaps_year = np.zeros_like(changemaps, dtype = int)\n",
    "    for i, year in enumerate(years_np):\n",
    "        changemaps_year[changemaps == i] = year\n",
    "    \n",
    "    \n",
    "    # Remove isolated pixels\n",
    "    updated_change_array, updated_probability_array = pp.remove_isolated_pixels(changemaps_year, accuracymaps)\n",
    "    \n",
    "    # Fill gaps and update probabilities\n",
    "    final_change_array, final_probability_array = pp.fill_small_holes_and_update_probabilities(updated_change_array, updated_probability_array) \n",
    "    \n",
    "    final_change_array = final_change_array.astype(float)\n",
    "    final_probability_array = final_probability_array.astype(float)\n",
    "    final_change_array[final_change_array ==0 ] = np.nan\n",
    "    final_probability_array[final_probability_array ==0 ] = np.nan\n",
    "    \n",
    "    \n",
    "    # Save output\n",
    "    output_filename_change = fm.joinpath(outpath, \"CD_2018_2019\")\n",
    "    output_filename_probability = fm.joinpath(outpath, \"prob_2018_2019\")\n",
    "    fm.write_shapefile(changemaps_year, geotransform, projection, output_filename_change)\n",
    "    fm.write_shapefile(accuracymaps, geotransform, projection, output_filename_probability)\n",
    "    \n",
    "                     \n",
    "    output_filename_process_change = fm.joinpath(outpath,\"CD_2018_2019_process\")\n",
    "    output_filename_process_probability = fm.joinpath(outpath,\"prob_2018_2019_process\")\n",
    "    fm.write_shapefile(final_change_array, geotransform, projection, output_filename_process_change) \n",
    "    fm.write_shapefile(final_probability_array, geotransform, projection, output_filename_process_probability)                 \n",
    "    \n",
    "    \n",
    "    print(\"Processing complete!\")    \n",
    "         \n",
    "\n",
    "#PREPARE SOME TOOLBOX PARAMETERS\n",
    "sensor = 'S2'\n",
    "tilename = 'T32TPS'\n",
    "years = ['2018','2019']\n",
    "maindir = '/home/mkhatereh/'\n",
    "datapath = '/home/mkhatereh/AIxPA/Code/Platform/DATA/'\n",
    "outpath = '/home/mkhatereh/AIxPA/Code/Platform/OUTPUT/'\n",
    "temppath = fm.joinpath(maindir, 'numpy')\n",
    "\n",
    "deforestation(sensor, tilename, years, maindir, datapath, outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d478cf61-a93e-49fc-ae1a-66257063763c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
